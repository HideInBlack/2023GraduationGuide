{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f9e8af0",
   "metadata": {},
   "source": [
    "# 对原数据集进行数据抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "adaf4069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import tokenize\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize # nltk.download('punkt') # 只下载一次即可\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c59f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取字符在字符串中最后出现的位置   \n",
    "def getNum(target, charStr):\n",
    "    \n",
    "    id = target\n",
    "    pos = id.find(charStr)    #此处id.find返回的参数为字符在其出现的位置\n",
    "    count = 0\n",
    "    while pos !=-1:\n",
    "        count = count + 1\n",
    "        # print('@出现在第{}个位置'.format(pos))\n",
    "        if count == 4:\n",
    "            return pos\n",
    "        pos = (id.find(charStr,pos+1))\n",
    "        \n",
    "# tokenize code分词(有一个缺陷是：不可以处理不完整代码 例如括号不匹配)\n",
    "def tokenize_code(origin_line):\n",
    "    \n",
    "    code_snippet = origin_line\n",
    "    # 将代码片段转换为字节流\n",
    "    code_bytes = code_snippet.encode('utf-8')\n",
    "    # 使用 tokenize 模块对代码片段进行分词\n",
    "    tokens = tokenize.tokenize(io.BytesIO(code_bytes).readline)\n",
    "    result_line = \"\"\n",
    "    # 输出每个 token 的类型和值\n",
    "    for token in tokens:\n",
    "        result_line = result_line + \" \" + token.string\n",
    "    return result_line\n",
    "    \n",
    "# tokenize 自然语言分词\n",
    "def tokenize_nlp(text):\n",
    "    result = \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        result = result + \" \" + token\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac93c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# # 使用nltk自然语言tokenizer 但是针对于代码效果不好 例子：'file.data.size'解析不出\n",
    "# # nltk.download('punkt') # 只下载一次即可\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text = \"\"\" newline_gradu KEEP  module.exports = function fileItem (props) { newline_gradu KEEP          } newline_gradu KEEP        </h4> newline_gradu KEEP        <div class=\"UppyDashboardItem-status\"> newline_gradu DEL         ${file.data.size && html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`} newline_gradu ADD         ${isNaN(file.data.size) ? '' : html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`} newline_gradu KEEP          ${file.source && html`<div class=\"UppyDashboardItem-sourceIcon\"> newline_gradu KEEP              ${acquirers.map(acquirer => { newline_gradu KEEP                if (acquirer.id === file.source) return html`<span title=\"${props.i18n('fileSource')}: ${acquirer.name}\">${acquirer.icon()}</span>`\"\"\"\n",
    "# tokens = word_tokenize(text)\n",
    "# print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bc4aba3",
   "metadata": {},
   "source": [
    "## commentsGenerate 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219c0622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据提取和special token处理等\n",
    "def data_text(json_path, code_path, msg_path, codeToken_path, msgToken_path, trainPair_path):\n",
    "    \n",
    "    file_code = open(code_path, 'w', encoding=\"utf-8\")\n",
    "    file_msg = open(msg_path, 'w', encoding=\"utf-8\")\n",
    "    file_codeToken = open(codeToken_path, 'w', encoding=\"utf-8\")\n",
    "    file_msgToken = open(msgToken_path, 'w', encoding=\"utf-8\")\n",
    "    file_trainPair = open(trainPair_path, 'w', encoding=\"utf-8\")\n",
    "    \n",
    "    with jsonlines.open(json_path) as reader:\n",
    "        for obj in reader:\n",
    "            if obj['msg'] != \"\":\n",
    "                target_index = getNum(obj['patch'], '@')\n",
    "                # 获取@最后一个位置使用字符串切片\n",
    "                temp_line = obj['patch'][target_index+1:]\n",
    "                # 数据预处理替换special token\n",
    "                temp_line = temp_line.replace(\"\\n+\", \" NEWLINE_NJUPT ADD_NJUPT \")\n",
    "                temp_line = temp_line.replace(\"\\n-\", \" NEWLINE_NJUPT DEL_NJUPT \")\n",
    "                temp_line = temp_line.replace(\"\\n\", \" NEWLINE_NJUPT KEEP_NJUPT \")\n",
    "                temp_line = temp_line.replace(\"\\t\", \" TABLE_NJUPT \")\n",
    "                if not temp_line.startswith(' NEW'):\n",
    "                    temp_line = \" NEWLINE_NJUPT KEEP_NJUPT \" + temp_line\n",
    "                file_code.write(temp_line + '\\n')\n",
    "                \n",
    "                file_msg.write(obj['msg'] + '\\n')\n",
    "                file_msgToken.write(tokenize_nlp(obj['msg']) + '\\n')\n",
    "                \n",
    "                # 下面是code tokenize化操作\n",
    "                result_line = \"\"\n",
    "                code_snippet = temp_line\n",
    "                code_bytes = code_snippet.encode('utf-8')\n",
    "                try:\n",
    "                    # 使用 tokenize 模块对代码片段进行分词\n",
    "                    tokens = tokenize.tokenize(io.BytesIO(code_bytes).readline)\n",
    "                    # 输出每个 token 的类型和值\n",
    "                    for token in tokens:\n",
    "                        result_line = result_line + \" \" + token.string\n",
    "                except tokenize.TokenError:\n",
    "                    pass\n",
    "                result_line = result_line[9:]\n",
    "                file_codeToken.write(result_line + '\\n')\n",
    "                file_trainPair.write(result_line + '\\t' + tokenize_nlp(obj['msg']) + '\\n')\n",
    "                \n",
    "\n",
    "        # 文件流关闭\n",
    "        file_code.close()\n",
    "        file_msg.close()\n",
    "        file_codeToken.close()\n",
    "        file_msgToken.close()\n",
    "        file_trainPair.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c51745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成code and comment 以及tokenize文件(测试数据集)\n",
    "data_text(\"G:\\\\now\\\\2023graduation\\\\authorCode\\\\数据集\\\\Comment_Generation\\\\msg-test.jsonl\", \n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt', '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msg.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt','..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_trainPair.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成code and comment 以及tokenize文件(训练数据集)\n",
    "data_text(\"G:\\\\now\\\\2023graduation\\\\authorCode\\\\数据集\\\\Comment_Generation\\\\msg-train.jsonl\", \n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_code.txt', '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_msg.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_codeTokens.txt','..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_msgTokens.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_trainPair.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecbcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成code and comment 以及tokenize文件(验证数据集)\n",
    "data_text(\"G:\\\\now\\\\2023graduation\\\\authorCode\\\\数据集\\\\Comment_Generation\\\\msg-valid.jsonl\", \n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\valid_code.txt', '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\valid_msg.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\valid_codeTokens.txt','..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\valid_msgTokens.txt',\n",
    "          '..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\valid_trainPair.txt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b141b2a",
   "metadata": {},
   "source": [
    "## commentsDenoise 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97131a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取到数据的总长度为：10169\n"
     ]
    }
   ],
   "source": [
    "\"\"\"数据读取code and comment\"\"\"\n",
    "# 从commentGeneration中得到code and comment的数据\n",
    "def read_data_nmt():\n",
    "    with open(os.path.join(\"..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\", 'test_trainPair.txt'), 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "# 读取code放到source里，读取comment数据放到target里\n",
    "source, target = d2l.tokenize_nmt(d2l.preprocess_nmt(read_data_nmt()), None)\n",
    "print(F\"读取到数据的总长度为：{len(target)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64cf4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mask comment 并生成数据\"\"\"\n",
    "for i in range(len(target)):\n",
    "    target[i] = target[i][1:len(target[i])]\n",
    "\n",
    "valid_tag = open('..\\\\dataTry\\\\codeReviewer\\\\commentsDenoise\\\\test_tag.txt', 'w', encoding=\"utf-8\")\n",
    "valid_comments = open('..\\\\dataTry\\\\codeReviewer\\\\commentsDenoise\\\\test_comments.txt', 'w', encoding=\"utf-8\")\n",
    "valid_trainPair = open('..\\\\dataTry\\\\codeReviewer\\\\commentsDenoise\\\\test_trainPair.txt', 'w', encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(target)):\n",
    "    tags = {}\n",
    "    # 取15%个随机数\n",
    "    temp_width = range(0,len(target[i]))\n",
    "    random_num = random.sample(temp_width, int(len(target[i]) * 0.15))\n",
    "    random_num.sort()\n",
    "    length_num = len(random_num)\n",
    "    # 若长度的15%为0 则默认mask第二个字符即可\n",
    "    if len(random_num) == 0:\n",
    "        length_num=1\n",
    "        random_num.append(1)\n",
    "    count = 0\n",
    "    for j in range(length_num):\n",
    "        tags.update({\"TAG\" + str(count): target[i][random_num[j]]})\n",
    "        target[i][random_num[j]] = \"TAG\" + str(count)\n",
    "        count = count + 1\n",
    "    temp_comment = \"\"\n",
    "    for n in range(len(target[i])): temp_comment = temp_comment + target[i][n] + \" \"\n",
    "    # 数据写入\n",
    "    valid_tag.write(str(tags) + '\\n')\n",
    "    valid_comments.write(temp_comment + '\\n')\n",
    "    valid_trainPair.write(temp_comment + '\\t' + str(tags) + '\\n')\n",
    "\n",
    "# 数据流关闭\n",
    "valid_tag.close()\n",
    "valid_comments.close()\n",
    "valid_trainPair.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
