{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloze Filling-基于BiLSTM的完形填空问题-2处预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----董政宇 2023年4月23日21:58:58-----\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"-----董政宇 2023年4月23日21:58:58-----\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step1: 数据预处理\n",
    "'''\n",
    "import os\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "\n",
    "# 超参数设置\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "NUM_HIDDENS = 256\n",
    "SEQ_LEN = 40\n",
    "WORD_DIM = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = '../dataTry/biLSTM_data'\n",
    "# chinese_data_path = os.path.join(path, 'chinese-data.txt')\n",
    "chinese_data_path = os.path.join(path, 'train.csv')\n",
    "origin_data,predict_word = [],[]\n",
    "# vocab\n",
    "vocab = []\n",
    "# str to index\n",
    "s2i = dict()\n",
    "# index to str\n",
    "i2s = dict()\n",
    "index = 0\n",
    "\n",
    "def get_vocab(path):\n",
    "    chinese_data_path = path\n",
    "    with open(chinese_data_path,'r', encoding='utf-8') as f:\n",
    "        # print('loading txt...')\n",
    "        for line in f:\n",
    "            if len(line) >= 41:\n",
    "            \t# 需要对？进行特殊处理\n",
    "                # print(line)\n",
    "                line = line.replace(\"?\", \"。\")\n",
    "                line = line.replace(\"？\", \"。\")\n",
    "                predict_word.append(line[20])\n",
    "                d = line[:20]+'?'+line[21:40]\n",
    "                origin_data.append(d)\n",
    "                for word in line:\n",
    "                    if word not in vocab:\n",
    "                        vocab.append(word)\n",
    "    f.close()\n",
    "get_vocab(chinese_data_path)\n",
    "\n",
    "# vocab size\n",
    "vocab.append('?')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "random.shuffle(vocab)\n",
    "s2i = {\n",
    "            word: i for i, word in enumerate(vocab)}\n",
    "i2s = {\n",
    "            i :word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4241, 2040, '藉', 8458, '1,\"入住的是度假区的豪华海景房,前台给?5楼(最高6楼),然后差不多100%的')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, s2i['?'], i2s[264], len(predict_word), origin_data[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1,《菲菲生气了》告诉孩子，生气是一种很?常的情绪反应，常常是由于自己感到受到了',\n",
       "  '0,标准内存太小，自己加了一个！没有自带?恢复光盘，重装系统后驱动有问题，自带的',\n",
       "  '1,\"不错,周围很热闹,出行方便,更不愁?吃饭的地方。去浅水湾、赤柱、海洋公园(',\n",
       "  '1,看过该书，感觉中医暂时不会消亡，尚有?、二十株老树活着，还有毛以林、黄煌、刘',\n",
       "  '0,这本书没读到底，不是特别喜欢。完全可?用序中的评价来表达我的感受：可以包容，',\n",
       "  '1,\"虽是观景房,不过我住的楼层太低(1?楼)看不到江景,但地点很好,离轻轨临江',\n",
       "  '1,性价比不错，交通方便。行政楼层感觉很?，只是早上8点楼上装修，好吵。 中餐厅',\n",
       "  '0,跟心灵鸡汤没什么本质区别嘛，至少我不?欢这样读经典，把经典都解读成这样有点去'],\n",
       " ['正', '的', '找', '一', '以', '9', '好', '喜'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin_data[8450:8458], predict_word[8450:8458]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 8425)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "step2: 数据集分割 数据迭代器构造\n",
    "'''\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self,data):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x,y = self.data[index]\n",
    "        # print(x,y)\n",
    "        x = [s2i[i] for i in x]\n",
    "        y = s2i[y]\n",
    "        return torch.LongTensor(x),torch.tensor(y)\n",
    "\n",
    "data = list(zip(origin_data,predict_word))\n",
    "# 随机打乱data_list顺序\n",
    "# random.shuffle(data)\n",
    "# 前8000条为训练数据集\n",
    "train_dataset = Dataset(data[:8425])\n",
    "# 8000条后为测试数据集\n",
    "test_dataset = Dataset(data[8425:])\n",
    "# 训练数据集迭代器\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    # 董政宇 改 2023年4月23日20:37:46\n",
    "    num_workers=0,#原num_workers=0，多线程？\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True\n",
    ")\n",
    "# 测试数据集迭代器\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    shuffle=False,\n",
    "    # 董政宇 改 2023年4月23日20:37:46\n",
    "    num_workers=0,#原num_workers=0，多线程？\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "len(test_dataset),len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8458\n",
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2797, 3566, 1605, 1435, 4113, 3892,  462, 2145, 3926,  227, 2786, 2540,\n",
       "         2633, 2125,  457, 1345, 3918, 3032, 2176,  950, 2040,  683,  952, 3553,\n",
       "         3849, 2563,  886, 3474, 3808, 1875, 3221,   10, 2753, 3105, 1892, 1270,\n",
       "         1435,  462, 3808, 3253]),\n",
       " 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "# data[:10]\n",
    "torch.tensor(264)\n",
    "x=\"1,看了《饮食定生死》，觉得它是一本通俗?懂的健康科普读物。从中我认识到了食物对\"\n",
    "print(len(x))\n",
    "x = [s2i[i] for i in x]\n",
    "torch.LongTensor(x), len(torch.LongTensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step3: BiLSTM双向长短期记忆网络定义\n",
    "'''\n",
    "EPOCH = 10\n",
    "import torch.nn.functional as F\n",
    "# 定义BiLSTM\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=WORD_DIM,\n",
    "            hidden_size=NUM_HIDDENS,\n",
    "            # 开启双向BiLSTM\n",
    "            bidirectional=True\n",
    "          )\n",
    "        self.out = nn.Linear(NUM_HIDDENS*2,vocab_size)\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        # x.shape [batch,seq_len,word_dim]     torch.Size([32, 40, 128])\n",
    "        # state.shape [1,seq_len,word_dim]\n",
    "        h_s = torch.randn((1*2,BATCH_SIZE, NUM_HIDDENS),device=device)\n",
    "        c_s = torch.randn((1*2,BATCH_SIZE, NUM_HIDDENS),device=device)\n",
    "        output,(_,_)= self.lstm(x.transpose(1,0),(h_s,c_s)) # output.shape [seq_len,batch,num_hiddens] torch.Size([40, 32, 256])\n",
    "        chinese = self.out(output[20])  # chinese.shape [batch,vocab_size]\n",
    "       \n",
    "        return chinese \n",
    "\n",
    "# 定义encoder       \n",
    "def encode(data,embed):\n",
    "    return embed(data)\n",
    "\n",
    "# 定义训练过程处理\n",
    "def train_porcess(pred,acc,device):\n",
    "    # pred.shape [batch,vocab_size]\n",
    "    # acc.shape [batch]\n",
    "    # print(pred.shape)torch.Size([32, 3195])\n",
    "\n",
    "    pred = F.softmax(pred,dim=-1)\n",
    "    # print(pred.shape)torch.Size([3195])\n",
    "    pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "    acc = acc.cpu().numpy()\n",
    "    print('预测下一个字:',[i2s[i] for i in pred.tolist()])\n",
    "    print('实际下一个字:',[i2s[i] for i in acc.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "step4: 实例化和初始化\n",
    "'''\n",
    "bilstm = BiLSTM(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(bilstm.parameters(),lr=LR)\n",
    "embed = nn.Embedding(vocab_size,WORD_DIM,device = device)\n",
    "output_model = 'models/BiLSTM_models/chineseBiLSTM_modelNew.pth'\n",
    "COUNT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "注意: 初始化模型参数（从头开始训练）\n",
    "'''\n",
    "torch.save(\n",
    "            {\n",
    "            \n",
    "                'model_state_dict': bilstm.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict()\n",
    "            }, output_model\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------epoch = 11--------\n",
      "loss = 3.3904199600219727\n",
      "loss = 4.206971645355225\n",
      "loss = 3.5998711585998535\n",
      "loss = 4.4049177169799805\n",
      "loss = 3.6020994186401367\n",
      "loss = 4.070418834686279\n",
      "loss = 3.4409632682800293\n",
      "loss = 4.064728260040283\n",
      "loss = 3.9831151962280273\n",
      "loss = 3.5937092304229736\n",
      "loss = 3.610607862472534\n",
      "loss = 3.584739923477173\n",
      "loss = 3.8011717796325684\n",
      "loss = 3.7854278087615967\n",
      "预测下一个字: ['的', '我', '是', '不', '是', '，', '的', '的', '有', '，', '一', '的', '是', '书', '。', '不', '是', '一', '不', '，', ' ', ',', '喜', '一', '间', '非', '，', ',', '是', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.1875\n",
      "--------epoch = 12--------\n",
      "loss = 3.0700008869171143\n",
      "loss = 3.2430179119110107\n",
      "loss = 3.6025044918060303\n",
      "loss = 3.364799737930298\n",
      "loss = 3.605013847351074\n",
      "loss = 4.072226524353027\n",
      "loss = 4.006681442260742\n",
      "loss = 3.566735029220581\n",
      "loss = 3.210050106048584\n",
      "loss = 3.7693073749542236\n",
      "loss = 3.878641128540039\n",
      "loss = 3.973186731338501\n",
      "loss = 4.100822448730469\n",
      "loss = 3.701484203338623\n",
      "预测下一个字: ['的', '我', '是', '不', '我', '，', '的', '的', '了', '，', '一', '的', '是', '书', '。', '不', '是', '一', '不', '，', ' ', ',', '喜', '一', '间', '非', '到', ',', '一', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.21875\n",
      "--------epoch = 13--------\n",
      "loss = 2.8971261978149414\n",
      "loss = 2.8870131969451904\n",
      "loss = 3.3137354850769043\n",
      "loss = 3.8094282150268555\n",
      "loss = 3.159797191619873\n",
      "loss = 3.1536102294921875\n",
      "loss = 3.5736124515533447\n",
      "loss = 3.251560688018799\n",
      "loss = 3.343651056289673\n",
      "loss = 3.081615924835205\n",
      "loss = 3.6466000080108643\n",
      "loss = 3.2305526733398438\n",
      "loss = 2.958894968032837\n",
      "loss = 3.3696460723876953\n",
      "预测下一个字: ['的', '个', '是', '不', '我', '，', '的', '！', '有', '，', '一', '的', '是', '书', '2', '不', '是', '一', '不', '，', ' ', ',', '喜', '一', '间', '非', '点', ',', '一', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.21875\n",
      "--------epoch = 14--------\n",
      "loss = 2.0427701473236084\n",
      "loss = 2.904402494430542\n",
      "loss = 2.967604398727417\n",
      "loss = 3.148754119873047\n",
      "loss = 2.699282646179199\n",
      "loss = 3.2911787033081055\n",
      "loss = 3.1024091243743896\n",
      "loss = 2.804490089416504\n",
      "loss = 3.2560362815856934\n",
      "loss = 2.743433952331543\n",
      "loss = 2.787294626235962\n",
      "loss = 3.344865083694458\n",
      "loss = 2.721541404724121\n",
      "loss = 3.2847273349761963\n",
      "预测下一个字: ['一', '个', '是', '不', '我', '，', '的', '！', '有', '，', '一', '的', '是', '本', '：', '不', '是', '一', '是', '，', ' ', '了', '喜', '一', '间', '非', '点', '说', '一', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.25\n",
      "--------epoch = 15--------\n",
      "loss = 2.8405909538269043\n",
      "loss = 2.9895811080932617\n",
      "loss = 2.4194960594177246\n",
      "loss = 2.4223601818084717\n",
      "loss = 2.494459629058838\n",
      "loss = 2.3878567218780518\n",
      "loss = 2.5968258380889893\n",
      "loss = 2.349876642227173\n",
      "loss = 2.574678897857666\n",
      "loss = 2.365811347961426\n",
      "loss = 2.870297431945801\n",
      "loss = 3.1517653465270996\n",
      "loss = 2.390507698059082\n",
      "loss = 2.991056442260742\n",
      "预测下一个字: ['的', '是', '是', '不', '我', '，', '的', '和', '的', '，', '一', '的', '是', '书', '：', '不', '是', '一', '，', '，', ' ', ',', '喜', '一', '间', '非', '到', '找', '的', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.25\n",
      "--------epoch = 16--------\n",
      "loss = 2.0689170360565186\n",
      "loss = 2.707244396209717\n",
      "loss = 2.447645664215088\n",
      "loss = 2.485710620880127\n",
      "loss = 2.5562193393707275\n",
      "loss = 2.4317760467529297\n",
      "loss = 1.8913360834121704\n",
      "loss = 2.526519298553467\n",
      "loss = 2.851304531097412\n",
      "loss = 2.7078213691711426\n",
      "loss = 2.6236984729766846\n",
      "loss = 2.3265137672424316\n",
      "loss = 2.2691290378570557\n",
      "loss = 2.718296766281128\n",
      "预测下一个字: ['，', '是', '是', '不', '我', '，', '的', '！', '有', '，', '一', '的', '可', '书', '：', '不', '是', '一', '，', '，', '舒', '了', '喜', '一', '间', '非', '到', '找', '！', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.28125\n",
      "--------epoch = 17--------\n",
      "loss = 2.225522756576538\n",
      "loss = 2.1579911708831787\n",
      "loss = 2.162879228591919\n",
      "loss = 2.36039400100708\n",
      "loss = 2.14216685295105\n",
      "loss = 1.9467674493789673\n",
      "loss = 1.974084734916687\n",
      "loss = 2.0949389934539795\n",
      "loss = 2.39747953414917\n",
      "loss = 2.2809243202209473\n",
      "loss = 1.9659403562545776\n",
      "loss = 2.426879405975342\n",
      "loss = 2.666306972503662\n",
      "loss = 2.607990026473999\n",
      "预测下一个字: ['一', '是', '是', '不', '我', '，', '不', '！', '的', '，', '一', '的', '是', '书', '：', '不', '是', '一', '不', '，', '舒', ',', '喜', '一', '间', '非', '到', '找', '的', '以', ',', '大']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.25\n",
      "--------epoch = 18--------\n",
      "loss = 2.1221587657928467\n",
      "loss = 1.8536542654037476\n",
      "loss = 2.121286392211914\n",
      "loss = 2.4106500148773193\n",
      "loss = 1.9686784744262695\n",
      "loss = 1.9128261804580688\n",
      "loss = 1.917210578918457\n",
      "loss = 1.9377235174179077\n",
      "loss = 2.0532150268554688\n",
      "loss = 1.7842422723770142\n",
      "loss = 2.0037078857421875\n",
      "loss = 2.049535036087036\n",
      "loss = 2.1034483909606934\n",
      "loss = 1.7730780839920044\n",
      "预测下一个字: ['一', '个', '是', '不', '个', '，', '的', '和', '了', '，', '一', '的', '可', '书', '：', '算', '是', '一', '不', '，', '舒', '了', '喜', '一', '间', '非', '到', '找', '！', '以', ',', '好']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.3125\n",
      "--------epoch = 19--------\n",
      "loss = 0.9138674736022949\n",
      "loss = 1.8625619411468506\n",
      "loss = 1.9136019945144653\n",
      "loss = 1.8866469860076904\n",
      "loss = 2.224870443344116\n",
      "loss = 1.447972059249878\n",
      "loss = 1.7858856916427612\n",
      "loss = 1.8696807622909546\n",
      "loss = 2.22265625\n",
      "loss = 1.8864716291427612\n",
      "loss = 1.6422053575515747\n",
      "loss = 2.1999354362487793\n",
      "loss = 2.126514434814453\n",
      "loss = 2.071697950363159\n",
      "预测下一个字: ['或', '个', '是', '不', '个', '就', '不', '和', '了', '来', '一', '的', '是', '书', '：', '算', '当', '一', '不', '，', '舒', '了', '喜', '一', '间', '非', '点', '找', '一', '以', ',', '大']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.34375\n",
      "--------epoch = 20--------\n",
      "loss = 1.5797855854034424\n",
      "loss = 1.7307716608047485\n",
      "loss = 1.938732624053955\n",
      "loss = 1.5735350847244263\n",
      "loss = 1.4855166673660278\n",
      "loss = 1.2811113595962524\n",
      "loss = 1.547141432762146\n",
      "loss = 1.736268162727356\n",
      "loss = 1.8294881582260132\n",
      "loss = 1.561003565788269\n",
      "loss = 1.8397345542907715\n",
      "loss = 1.6327528953552246\n",
      "loss = 1.4788953065872192\n",
      "loss = 1.5756103992462158\n",
      "预测下一个字: ['或', '个', '是', '不', '我', '，', '不', '和', '了', '，', '一', '的', '可', '书', '：', '算', '当', '一', '，', '，', '舒', '了', '喜', '一', '间', '非', '到', '找', '一', '以', ',', '大']\n",
      "实际下一个字: ['或', '了', '是', '午', '本', '见', '真', '失', '多', '级', '所', '走', ',', '不', '：', '算', '害', '线', '妹', '，', '舒', '舒', '喜', '廖', '间', '正', '的', '找', '一', '以', '9', '好']\n",
      "测试数据集的accuracy为：\n",
      " 0.34375\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "step5: 开始训练(继续训练)\n",
    "'''\n",
    "BATCH_SIZE =32\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    '''这段注释掉的代码意义何在 研究ing'''\n",
    "    bestscore = 0\n",
    "    # 加载模型（接上次训练参数继续训练？）\n",
    "    checkpoint = torch.load(output_model,map_location='cpu')\n",
    "    # print(checkpoint)\n",
    "    bilstm.load_state_dict(checkpoint['model_state_dict'],strict=False)\n",
    "    bilstm.to(device)\n",
    "    bilstm.train()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # print(f'epoch = {epoch}')\n",
    "        print(f'--------epoch = {COUNT*10 + epoch + 1}--------') \n",
    "        accuracy =0\n",
    "        time=0\n",
    "        # 训练数据集开始训练并计算loss损失  \n",
    "        for step,datas in enumerate(train_loader):\n",
    "            \n",
    "            data,label = tuple(t.to(device)for t in datas)\n",
    "            data = encode(data,embed)\n",
    "            chinese = bilstm(data)\n",
    "            loss = criterion(chinese,label.long())\n",
    "\n",
    "            # # 测试\n",
    "            # # print(data)\n",
    "            # # print(label)\n",
    "            # print(\"-------------\")\n",
    "            # print(chinese)\n",
    "            # print(\"-------*****************------\")\n",
    "            # pred = F.softmax(chinese,dim=-1)\n",
    "            # print(pred)\n",
    "            # print(pred.argmax(dim=-1))\n",
    "            # # print(pred.shape)torch.Size([3195])返回指定维度最大值的序号\n",
    "            # pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "            # acc = label.cpu().numpy()\n",
    "            # print(label)\n",
    "            # print('预测下一个字:',[i2s[i] for i in pred.tolist()])\n",
    "            # print('实际下一个字:',[i2s[i] for i in acc.tolist()])\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if step%20 == 0:\n",
    "                print(f'loss = {loss}')\n",
    "                \n",
    "            # if step%50 == 0:\n",
    "            #     train_porcess(chinese,label,device)\n",
    "            # break\n",
    "        \n",
    "        bilstm.eval()  \n",
    "        # 测试数据集开始评估并计算accuracy  \n",
    "        for step,datas in enumerate(test_loader):\n",
    "            # print(datas)\n",
    "            data,label = tuple(t.to(device)for t in datas)\n",
    "            data = encode(data,embed)\n",
    "            chinese = bilstm(data)\n",
    "            pred = F.softmax(chinese,dim=-1)       \n",
    "            pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "            acc = label.cpu().numpy()\n",
    "            accuracy += sum(acc==pred)\n",
    "            time += len(acc)\n",
    "\n",
    "        # 测试数据集预测\n",
    "        print('预测下一个字:',[i2s[i] for i in pred.tolist()])\n",
    "        print('实际下一个字:',[i2s[i] for i in acc.tolist()])\n",
    "        # 测试数据集的精确度\n",
    "        accuracy/=time+0.0\n",
    "        print('测试数据集的accuracy为：\\n',accuracy)\n",
    "        #保存最优模型的参数（得分最高的模型参数）\n",
    "        if accuracy>bestscore:\n",
    "            torch.save(\n",
    "            {\n",
    "            \n",
    "                'model_state_dict': bilstm.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict()\n",
    "            }, output_model\n",
    "          )\n",
    "            bestscore = accuracy\n",
    "        bilstm.train()\n",
    "\n",
    "        # break\n",
    "    COUNT = COUNT + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
