{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "645cb342",
   "metadata": {},
   "source": [
    "## Use Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc68409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213e9725",
   "metadata": {},
   "source": [
    "## 1.1 (valid-256)模型加载-废"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620b91dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (embedding): Embedding(9407, 128)\n",
       "    (rnn): GRU(128, 256, num_layers=2, dropout=0.1)\n",
       "  )\n",
       "  (decoder): Seq2SeqAttentionDecoder(\n",
       "    (attention): AdditiveAttention(\n",
       "      (W_k): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (W_q): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (w_v): Linear(in_features=256, out_features=1, bias=False)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (embedding): Embedding(2830, 128)\n",
       "    (rnn): GRU(384, 256, num_layers=2, dropout=0.1)\n",
       "    (dense): Linear(in_features=256, out_features=2830, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复原训练时超参数\n",
    "embed_size, num_hiddens, num_layers, dropout = 128, 256, 2, 0.1\n",
    "batch_size, num_steps = 64, 150\n",
    "device = d2l.try_gpu()\n",
    "\n",
    "# 训练时数据集数量：None， 数据集：'valid_trainPair.txt' , min_freq=\"5\"\n",
    "train_iter, src_vocab, tgt_vocab = d2l.model_load_data_nmt(batch_size, num_steps, None, data_name='valid_trainPair.txt', min_freq=5)\n",
    "\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = d2l.Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# 模型的导入\n",
    "clone = d2l.EncoderDecoder(encoder, decoder)\n",
    "clone.load_state_dict(torch.load('models/seq2seq_commentsG256.params'))\n",
    "clone.to(device) # 把模型数据放在GPU上去"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3df8dc0",
   "metadata": {},
   "source": [
    "## 1.2 valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c38bc9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************************\n",
      " \n",
      "  module.exports = function fileItem (props) { \n",
      "          } \n",
      "        </h4> \n",
      "        <div class=\"UppyDashboardItem-status\"> \n",
      "-         ${file.data.size && html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`} \n",
      "+         ${isNaN(file.data.size) ? '' : html`<div class=\"UppyDashboardItem-statusSize\">${prettyBytes(file.data.size)}</div>`} \n",
      "          ${file.source && html`<div class=\"UppyDashboardItem-sourceIcon\"> \n",
      "              ${acquirers.map(acquirer => { \n",
      "                if (acquirer.id === file.source) return html`<span title=\"${props.i18n('fileSource')}: ${acquirer.name}\">${acquirer.icon()}</span>` \n",
      "\n",
      "==>True comment:  We are trying to support IE 10-11 , so we 'll need a polyfill for this one , I think .\n",
      "==>Predicted comment: I think this is a <unk> change , but I am wondering if we 're using ` <unk> ` obj , and why disable those side ?,  \n",
      "==>bleu 0.234 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  class S3KeyTest(unittest.TestCase): \n",
      "          remote_metadata = check._get_remote_metadata() \n",
      "   \n",
      "          # TODO: investigate whether encoding ' ' as '%20' makes sense \n",
      "-         self.assertEqual(check.cache_control, 'public,%20max-age=500') \n",
      "+         self.assertIn( \n",
      "+             check.cache_control, \n",
      "+             ('public,%20max-age=500', 'public, max-age=500') \n",
      "+         ) \n",
      "          self.assertEqual(remote_metadata['cache-control'], 'public,%20max-age=500') \n",
      "          self.assertEqual(check.get_metadata('test-plus'), 'A plus (+)') \n",
      "          self.assertEqual(check.content_disposition, 'filename=Sch%C3%B6ne%20Zeit.txt') \n",
      "\n",
      "==>True comment:  This change looks unrelated to the CL description ?\n",
      "==>Predicted comment: This is a <unk> change , but I 'm not sure why this is needed .,  \n",
      "==>bleu 0.210 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  func TestFSRepoInit(t *testing.T) { \n",
      "   \n",
      "   \t dir, err := ioutil.TempDir(\"\", \"\") \n",
      "   \t assert.NoError(t, err) \n",
      "-  \n",
      "-  \t defer os.RemoveAll(dir) \n",
      "+  \t defer func() { \n",
      "+  \t  \t require.NoError(t, os.RemoveAll(dir)) \n",
      "+  \t }() \n",
      "   \n",
      "   \t t.Log(\"init FSRepo\") \n",
      "   \t assert.NoError(t, InitFSRepo(dir, config.NewDefaultConfig())) \n",
      "\n",
      "==>True comment:  It would be worth factoring this out to a function , but you do n't have to do that here .\n",
      "==>Predicted comment: This is a <unk> change , but I 'm not sure why this is included .,  \n",
      "==>bleu 0.221 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  def execute_reentrant_pipeline(pipeline, typed_environment, throw_on_error, reen \n",
      "  def get_subset_pipeline(pipeline, solid_subset): \n",
      "      check.inst_param(pipeline, 'pipeline', PipelineDefinition) \n",
      "      check.opt_list_param(solid_subset, 'solid_subset', of_type=str) \n",
      "-     return pipeline if solid_subset is None else build_sub_pipeline(pipeline, solid_subset) \n",
      "+     return pipeline if not solid_subset else build_sub_pipeline(pipeline, solid_subset) \n",
      "   \n",
      "   \n",
      "  def create_typed_environment(pipeline, environment=None): \n",
      "\n",
      "==>True comment:  this is a behavior change . solid_subset= [ ] represents an empty pipeline where as solid_subset=None is the full pipeline\n",
      "==>Predicted comment: I think this is a <unk> change , but I am wondering if we 're using ` <unk> ` obj , we should probably make it assumption to make our fields .,  \n",
      "==>bleu 0.213 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  Licensed under the MIT License. See License.txt in the project root for license \n",
      "              { \n",
      "                  foreach (var unmatchedSetting in CustomSettings.Keys) \n",
      "                  { \n",
      "-                     Logger.LogError(new ArgumentException(unmatchedSetting), \n",
      "-                         Resources.ParameterIsNotValid, unmatchedSetting); \n",
      "+                     Logger.LogWarning(Resources.ParameterIsNotValid, unmatchedSetting); \n",
      "                  } \n",
      "              } \n",
      "              ErrorManager.ThrowErrors(); \n",
      "\n",
      "==>True comment:  this is a breaking change , any specific reason you want to do this ?\n",
      "==>Predicted comment: This is a <unk> change , but I 'm not sure why this is needed .,  \n",
      "==>bleu 0.391 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  type TaskConfig struct { \n",
      "   \t Count          Count                `yaml:\"count\"` \n",
      "   \t ExecuteCommand ExecuteCommand       `yaml:\"exec\"` \n",
      "   \t Variables      map[string]string    `yaml:\"variables\"` \n",
      "+  \t EnvFile        string               `yaml:\"env_file\"` \n",
      "   \t Secrets        map[string]string    `yaml:\"secrets\"` \n",
      "   \t Storage        Storage              `yaml:\"storage\"` \n",
      "  } \n",
      "\n",
      "==>True comment:  A question ! I think ` string ` totally works , but what do you think of ` * string ` for consistency ?\n",
      "==>Predicted comment: I think this is a <unk> change , but I am wondering if we 're using ` <unk> ` obj , and why disable those side ?,  \n",
      "==>bleu 0.279 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  import org.springframework.context.annotation.AnnotationConfigApplicationContext \n",
      "  public class JavaFXApplication extends Application { \n",
      "   \n",
      "      public static void main(String[] args) { \n",
      "-         Application.launch(JavaFXApplication.class); \n",
      "+         try { \n",
      "+             Application.launch(JavaFXApplication.class); \n",
      "+         } catch (Exception e) { \n",
      "+             e.printStackTrace(); \n",
      "+         } \n",
      "      } \n",
      "   \n",
      "      @Override \n",
      "\n",
      "==>True comment:  I think we should log the exception to a log file instead of printing it . The current behavior ( not catching the exception ) should lead to an automatic print of the exception to the terminal/console .\n",
      "==>Predicted comment: This is a <unk> change , but I 'm not sure if we should catch the ` <unk> ` method . It seems like ` <unk> ` method , but I 'm not sure what it is <unk> .,  \n",
      "==>bleu 0.200 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  import java.util.Map; \n",
      "   * @param <F> the concrete Java class of a ContentFile instance. \n",
      "   */ \n",
      "  public interface ContentFile<F> { \n",
      "+   /** \n",
      "+    * Returns the ordinal position of the file in a manifest, or null if it was not read from a manifest. \n",
      "+    */ \n",
      "+   Long pos(); \n",
      "+  \n",
      "    /** \n",
      "     * Returns id of the partition spec used for partition metadata. \n",
      "     */ \n",
      "\n",
      "==>True comment:  qq : do we want to include anything in the name to indicate that it is a position in the manifest ?\n",
      "==>Predicted comment: This is a temporary change , but it should be done in the main impl .,  \n",
      "==>bleu 0.248 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  def define_environment_cls(pipeline_def): \n",
      "      ) \n",
      "   \n",
      "   \n",
      "+ def context_cls_inst(pipeline_def): \n",
      "+     check.inst_param(pipeline_def, 'pipeline_def', PipelineDefinition) \n",
      "+     pipeline_name = camelcase(pipeline_def.name) \n",
      "+     return SystemNamedDict( \n",
      "+         name='{pipeline_name}.Context'.format(pipeline_name=pipeline_name), \n",
      "+         fields={ \n",
      "+             'context': define_maybe_optional_selector_field( \n",
      "+                 define_context_context_cls(pipeline_name, pipeline_def.context_definitions) \n",
      "+             ) \n",
      "+         }, \n",
      "+     ).inst() \n",
      "+  \n",
      "+  \n",
      "  def define_expectations_config_cls(name): \n",
      "      check.str_param(name, 'name') \n",
      "   \n",
      "\n",
      "==>True comment:  the naming convention I 'm been adopting if ` _type ` for instances of these classes . So maybe ` context_config_type ` is a better name for this fn\n",
      "==>Predicted comment: I think this is a <unk> change , but I am wondering if we 're using ` <unk> ` obj , we should probably make it assumption to make it assumption .,  \n",
      "==>bleu 0.220 \n",
      "\n",
      "*****************************************************************************************************\n",
      " \n",
      "  public class MoveIT { \n",
      "          linkDataset.then().assertThat() \n",
      "                  .statusCode(OK.getStatusCode()); \n",
      "   \n",
      "+         // A dataset cannot be linked to the same dataverse again. \n",
      "+         Response tryToLinkAgain = UtilIT.linkDataset(datasetPid, dataverse2Alias, superuserApiToken); \n",
      "+         tryToLinkAgain.prettyPrint(); \n",
      "+         tryToLinkAgain.then().assertThat() \n",
      "+                 .statusCode(FORBIDDEN.getStatusCode()); \n",
      "+  \n",
      "          Response getLinksBefore = UtilIT.getDatasetLinks(datasetPid, superuserApiToken); \n",
      "          getLinksBefore.prettyPrint(); \n",
      "          getLinksBefore.then().assertThat() \n",
      "\n",
      "==>True comment:  is this test in the move tests ? I see what you mean then - it works , but I wonder if we wo n't lose track that it 's being tested here .\n",
      "==>Predicted comment: This is a <unk> change , but I 'm not sure if we should catch the ` <unk> ` method . It seems like ` <unk> ` , but I think it should be good to have ` <unk> ` , eventually .,  \n",
      "==>bleu 0.257 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6024\\3892322212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msum_blue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_code\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormal_codes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpredict_comment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_attention_weight_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_seq2seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0msum_blue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum_blue\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_comment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\PythonTool\\lib\\site-packages\\d2l\\torch.py\u001b[0m in \u001b[0;36mpredict_seq2seq\u001b[1;34m(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[0moutput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weight_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdec_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m         \u001b[1;31m# We use the token with the highest prediction likelihood as the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;31m# of the decoder at the next time step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\PythonTool\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\PythonTool\\lib\\site-packages\\d2l\\torch.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, state)\u001b[0m\n\u001b[0;32m   1315\u001b[0m             \u001b[1;31m# query的形状为(batch_size,1,num_hiddens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;31m# 这里的x就是一个token 所以num_step都是1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1317\u001b[1;33m             \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m             \u001b[1;31m# context的形状为(batch_size,1,num_hiddens)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m             \u001b[1;31m# 这里的context就已经是找到最相关的hidden state了\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.2:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39820c4b",
   "metadata": {},
   "source": [
    "## 2.1 模型加载5k_128.params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9541fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复原训练时超参数\n",
    "embed_size, num_hiddens, num_layers, dropout = 128, 128, 2, 0.1\n",
    "batch_size, num_steps = 64, 150\n",
    "device = d2l.try_gpu()\n",
    "\n",
    "# 训练时数据集数量：5000， 数据集：'valid_trainPair.txt' , min_freq=\"5\"\n",
    "train_iter, src_vocab, tgt_vocab = d2l.model_load_data_nmt(batch_size, num_steps, 5000, data_name='valid_trainPair.txt', min_freq=5)\n",
    "\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = d2l.Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# 模型的导入\n",
    "clone2 = d2l.EncoderDecoder(encoder, decoder)\n",
    "clone2.load_state_dict(torch.load('models/seq2seq_commentsG5k_128.params'))\n",
    "clone2.to(device) # 把模型数据放在GPU上去"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1f0a0e",
   "metadata": {},
   "source": [
    "## 2.2 valid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b806d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone2, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.2:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b522231a",
   "metadata": {},
   "source": [
    "## 3.1 模型加载 2w_128.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a94b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复原训练时超参数\n",
    "embed_size, num_hiddens, num_layers, dropout = 128, 128, 2, 0.1\n",
    "batch_size, num_steps = 64, 100\n",
    "device = d2l.try_gpu()\n",
    "\n",
    "# 训练时数据集数量：2w， 数据集：'train_trainPair.txt' , min_freq=\"5\"\n",
    "train_iter, src_vocab, tgt_vocab = d2l.model_load_data_nmt(batch_size, num_steps, 20000, data_name='train_trainPair.txt', min_freq=5)\n",
    "\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = d2l.Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# 模型的导入\n",
    "clone3 = d2l.EncoderDecoder(encoder, decoder)\n",
    "clone3.load_state_dict(torch.load('models/seq2seq_commentsG2w_128.params'))\n",
    "clone3.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "926dca46",
   "metadata": {},
   "source": [
    "## 3.2 valid test 2w_128.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone3, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.3:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba507e48",
   "metadata": {},
   "source": [
    "## 4.1 模型加载 2.5w_256.params-废"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复原训练时超参数\n",
    "embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.1\n",
    "batch_size, num_steps = 64, 150\n",
    "device = d2l.try_gpu()\n",
    "\n",
    "# 训练时数据集数量：2.5w， 数据集：'train_trainPair.txt' , min_freq=\"5\"\n",
    "train_iter, src_vocab, tgt_vocab = d2l.model_load_data_nmt(batch_size, num_steps, 25000, data_name='train_trainPair.txt', min_freq=5)\n",
    "\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = d2l.Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# 模型的导入\n",
    "clone4 = d2l.EncoderDecoder(encoder, decoder)\n",
    "clone4.load_state_dict(torch.load('models/seq2seq_commentsG2.5w_256.params'))\n",
    "clone4.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b2c3acd",
   "metadata": {},
   "source": [
    "## 4.2 valid test 2.5w_256.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone4, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.3:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "050027a3",
   "metadata": {},
   "source": [
    "## 5.1 参数微调2w_256_0.5.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 复原训练时超参数\n",
    "embed_size, num_hiddens, num_layers, dropout = 128, 256, 2, 0.5\n",
    "batch_size, num_steps = 128, 150\n",
    "lr, num_epochs, device = 0.003, 50, d2l.try_gpu()\n",
    "\n",
    "# 训练时数据集数量：2.5w， 数据集：'train_trainPair.txt' , min_freq=\"5\"\n",
    "train_iter, src_vocab, tgt_vocab = d2l.model_load_data_nmt(batch_size, num_steps, 20000, data_name='train_trainPair.txt', min_freq=5)\n",
    "\n",
    "encoder = d2l.Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = d2l.Seq2SeqAttentionDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "\n",
    "# 模型的导入\n",
    "clone4 = d2l.EncoderDecoder(encoder, decoder)\n",
    "clone4.load_state_dict(torch.load('models/seq2seq_commentsG2w_256_0.5.params'))\n",
    "clone4.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c16e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\test_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone4, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.3:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_codeTokens.txt')\n",
    "comments = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_msgTokens.txt')\n",
    "normal_codes = d2l.model_read_test_data('..\\\\dataTry\\\\codeReviewer\\\\commentsGeneration\\\\train_code.txt')\n",
    "\n",
    "sum_blue = 0\n",
    "for code, comment, normal_code in zip(codes, comments, normal_codes):\n",
    "    predict_comment, dec_attention_weight_seq = d2l.predict_seq2seq(clone4, code, src_vocab, tgt_vocab, num_steps, device, True)\n",
    "    sum_blue = sum_blue + round(d2l.bleu(predict_comment, comment, k=2), 3)\n",
    "    \n",
    "    if d2l.bleu(predict_comment, comment, k=2) >= 0.3:\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT ADD_NJUPT\", \"\\n+\", )\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT DEL_NJUPT\", \"\\n-\",)\n",
    "        normal_code = normal_code.replace(\"NEWLINE_NJUPT KEEP_NJUPT\", \"\\n\")\n",
    "        normal_code = normal_code.replace(\"TABLE_NJUPT\", \"\\t\")\n",
    "        \n",
    "        print(\"*****************************************************************************************************\")\n",
    "        print(f'{normal_code} \\n\\n==>True comment: {comment}\\n==>Predicted comment:{predict_comment}, ',\n",
    "              f'\\n==>bleu {d2l.bleu(predict_comment, comment, k=2):.3f} \\n')\n",
    "        \n",
    "print('平均blue: {:.3f}'.format(sum_blue/len(codes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c612b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
